---
layout: default
title: john so
---

<div class="section">
  <div class="subsection">
    <button onclick="dark()">
      <div class="title-img">
          <img src="/media/img/headshot.jpeg" alt="me!">
          <script>
            function dark() {
              let element = document.body;
              element.classList.toggle("light");
            }
            </script>
      </div>
    </button>
    <div class="text-container title-text">
      <h1>john so</h1>
      <p><i>johnso &lt;at&gt; stanford &lt;dot&gt; edu</i></p>
      <p> &#123;
        <a target="_blank" href="https://github.com/johnrso">github</a> |
        <a target="_blank" href="https://twitter.com/johnrso_">twitter</a> |
        <a target="_blank" href="https://www.linkedin.com/in/johnianrso/">linkedin</a> |
        <a target="_blank" href="/media/pdf/johnso_2023.pdf">resume</a>
      &#125; </p>
    </div>
  </div>
  <div class="text-container">
    <p>
      I am a MS CS student at <a target="_blank" href="https://www.stanford.edu/">Stanford University</a>, where I'm fortunate to be advised by
      <a target="_blank" href="https://shurans.github.io/">Shuran Song</a> as a member of <a target="_blank" href="https://real.stanford.edu/">REAL</a> (Robotics and Embodied Artificial Intelligence Lab) @ Stanford.
    <!-- </p><br>
    <p> -->
      I will also be an AI Resident at <a target="_blank" href="https://www.1x.tech/about">1X</a> this summer working on humanoid robotics.
    <!-- </p><br>
    <p> -->
      Previously, I finished my BS EECS at <a target="_blank" href="https://engineering.berkeley.edu/">UC Berkeley</a>, where I was fortunate to be advised by
      <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
      <a target="_blank" href="https://stepjam.github.io/">Stephen James</a>, and
      <a target="_blank" href="https://xingyu-lin.github.io/">Xingyu Lin</a> as a part of Berkeley's
      <a target="_blank" href="https://rll.berkeley.edu/">Robot Learning Lab</a>.
    </p>
  </div>
</div>
<div class="section">
  <div class="text-container">
    <h2>research</h2>
  </div>
  <div class="subsection">
    <div class="text-container">
      <p>
        My dream is for robots to become an everyday household occurrence; an important step is to enable robots to quickly adapt prior knowledge to new scenes or tasks. Towards this, I aim to answer two questions:
      </p>
      <ol>
        <li>How do we pre-train using out-of-distribution data, such as cross-embodiment demos or videos?</li>
        <li>How do we use in-domain robotics data to enable sample-efficient policy learning?</li>
      </ol>
      <p>
        My recent interests are in capturing useful priors from multi-task video datasets for robot learning, such as motion and skills. Long term, I hope to leverage perspectives from cognitive science to inform how robots can learn from and like humans.
      </p>
    </div>
  </div>
  <div class="subsection">
    <div class="text-container">
      <h3>Any-point Trajectory Modeling for Policy Learning</h3>
      <p>
        <a target="_blank" href="https://alvinwen428.github.io/">Chuan Wen</a>*,
        <a target="_blank" href="https://xingyu-lin.github.io/">Xingyu Lin</a>*,
        <b><u>John So</u></b>*,
        <a target="_blank" href="https://www.cse.cuhk.edu.hk/~qdou/">Qi Dou</a>,
        <a target="_blank" href="https://ck-kai.github.io/">Kai Chen</a>,
        <a target="_blank" href="https://yang-gao.weebly.com/">Yang Gao</a>,
        <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
      </p>
      <p>
        We learn to predict the future trajectories of arbitrary points by pre-training on actionless videos. Using these trajectories for downstream policy learning, we demonstrate sample-efficient learning and cross-embodiment knowledge transfer.
      </p>
      <p>
        &#123; <a target="_blank" href="/media/pdf/atm.pdf">paper</a> |
        <a target="_blank" href="https://arxiv.org/abs/2401.00025" >arXiv</a> |
        <a target="_blank" href="https://xingyu-lin.github.io/atm/">website</a>
        <!-- <a target="_blank" href="https://github.com/johnrso/spawnnet">code</a>  -->
        &#125;
      </p>
    </div>
    <a target="_blank" href="https://xingyu-lin.github.io/atm/">
      <div class="project-media">
        <video src="/media/vid/atm.mp4" autoplay muted inline loop>
          Your browser does not support the video tag.
        </video>
      </div>
    </a>
  </div>
  <div class="subsection">
    <div class="text-container">
      <h3>SpawnNet: Learning Generalizable Visuomotor Skills from Pre-trained Networks</h3>
      <p>
        <a target="_blank" href="https://xingyu-lin.github.io/">Xingyu Lin</a>*,
        <b><u>John So</u></b>*,
        <a target="_blank" href="https://sashwat-mahalingam.github.io">Sashwat Mahalingam</a>,
        <a target="_blank" href="https://fangchenliu.github.io">Fangchen Liu</a>,
        <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>
      </p>
      <p>
        We extract dense features from pre-trained networks to learn generalizable manipulation skills.
        This shows improvements on categorical generalization against paradigms such as naively using
        pre-trained representations.
      </p>
      <p>
        <span class="badge conference"><b>ICRA 2024</b></span>
        &#123; <a target="_blank" href="/media/pdf/spawnnet.pdf">paper</a> |
        <a target="_blank" href="https://arxiv.org/abs/2307.03567" >arXiv</a> |
        <a target="_blank" href="https://xingyu-lin.github.io/spawnnet/">website</a> |
        <a target="_blank" href="https://github.com/johnrso/spawnnet">code</a> &#125;
      </p>
    </div>
    <a target="_blank" href="https://xingyu-lin.github.io/spawnnet/">
      <div class="project-media">
        <video src="/media/vid/spawnnet.mp4" autoplay muted inline loop>
          Your browser does not support the video tag.
        </video>
      </div>
    </a>
  </div>
  <div class="subsection">
    <div class="text-container">
      <h3>Sim-to-Real via Sim-to-Seg: End-to-end Off-road Autonomous Driving Without Real Data</h3>
      <p>
        <b><u>John So</u></b>*,
        <a target="_blank" href="https://amberxie88.github.io/" >Amber Xie</a>*,
        <a target="_blank" href="https://www-robotics.jpl.nasa.gov/who-we-are/people/sunggoo-jung/" >Sunggoo Jung</a>,
        <a target="_blank" href="https://www-robotics.jpl.nasa.gov/who-we-are/people/jeffrey_edlund/" >Jeffrey Edlund</a>,
        <a target="_blank" href="https://www-robotics.jpl.nasa.gov/who-we-are/people/rohan_thakker/" >Rohan Thakker</a>,
        <a target="_blank" href="https://aliagha.site/" >Ali Agha-mohammadi</a>,
        <a target="_blank" href="https://people.eecs.berkeley.edu/~pabbeel/" >Pieter Abbeel</a>,
        <a target="_blank" href="https://stepjam.github.io/" >Stephen James</a>
      </p>
      <p>
        We use photorealistic simulation to learn a segmentation model and train a navigation policy with RL in the learned segmentation space.
        We deploy zero-shot to a real vehicle.
      </p>
      <p>
        <span class="badge conference"><b>CoRL 2022</b></span>
        &#123; <a target="_blank" href="/media/pdf/sim-to-seg.pdf" >paper</a> |
        <a target="_blank" href="https://arxiv.org/abs/2210.14721">arXiv</a> |
        <a target="_blank" href="https://sites.google.com/view/sim2segcorl2022/home">website</a> |
        <a target="_blank" href="https://github.com/rll-research/sim2seg">code</a> &#125;
      </p>
    </div>
    <a target="_blank" href="https://sites.google.com/view/sim2segcorl2022/home">
      <div class="project-media">
        <video src="/media/vid/sim2seg.mp4" autoplay muted inline loop>
          Your browser does not support the video tag.
        </video>
      </div>
    </a>
  </div>
</div>
<div class="section">
  <div class="text-container">
    <h2>teaching</h2>
  </div>
  <div class="subsection">
    <div class="text-container">
      <p><b><a target="_blank" href="https://cs229.stanford.edu/">CS 229: Machine Learning</a></b>: Winter 2024</p>
      <p><b><a target="_blank" href="https://people.eecs.berkeley.edu/~jrs/189/">CS 189: Introduction to Machine Learning</a></b>: Spring 2023</p>
      <p><b><a target="_blank" href="https://cs61a.org/">CS 61A: Structure and Interpretation of Computer Programs</a></b>: Fall 2022, Spring 2022 (Head TA), Fall 2021</p>
    </div>
  </div>
</div>
<div class="section">
  <div class="text-container">
    <h2>miscellaneous</h2>
  </div>
  <div class="subsection">
    <div class="text-container">
      <p>
        As an undergrad, I spent the majority of my time outside of research helping to build, organize, and
        lead <a target="_blank" href="https://ml.berkeley.edu/" >Machine Learning at Berkeley (ML@B)</a>,
        serving as the organization's president in Fall 2022. We presented a white paper about our structure
        and initiatives at the NeurIPS 2022 <a target="_blank" href="https://sites.google.com/view/broadening-collaboration-in-ml/home">Broadening Research Collaborations in ML Workshop</a>;
        you may find a preprint <a target="_blank" href="/media/pdf/built_to_last.pdf">here</a>.
      </p>
      <br>
      <p>
        I like to think about how to best teach, learn, and optimize for fulfillment. Shoot me an email or DM if you'd like to chat :&#41;
      </p>
    </div>
  </div>
</div>

<div class="section footer">
  <p>last updated: {{ site.time | date_to_string }} | <a href="#">&#128640;</a></p>
</div>